NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", header=FALSE)
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=NeuronAcc$Structure, col = 'blue')
LayerAcc.bar <- barplot(LayerAcc$V2,
names.arg=LayerAcc$V1, col = 'blue')
View(AccLayer)
View(LayerAcc)
StructureAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
View(StructureAcc)
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy,
names.arg=NeuronAcc$Structure, col = 'red')
LayerAcc <- read.csv("~/HeartDisease/LayerAcc.csv", header=FALSE)
LayerAcc.bar <- barplot(LayerAcc$V2, names.arg=LayerAcc$V1, col = 'blue', xlab = "Accuracy", ylab = "Number of hidden neurons")
LayerAcc <- read.csv("~/HeartDisease/LayerAcc.csv", header=FALSE)
LayerAcc.bar <- barplot(LayerAcc$V2, names.arg=LayerAcc$V1, col = 'blue', xlab = "Accuracy", ylab = "Number of hidden neurons")
LayerAcc <- read.csv("~/HeartDisease/LayerAcc.csv", header=FALSE)
LayerAcc.bar <- barplot(LayerAcc$V2, names.arg=LayerAcc$V1, col = 'blue', xlab = "Number of hidden neurons", ylab = "Accuracy")
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=NeuronAcc$Structure, col = 'blue')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=NeuronAcc$Structure, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:25
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:24
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:24
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:24
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:16
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:16
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
itr = 1:15
NeuraonAcc.bar <- barplot(NeuronAcc$Accuracy, names.arg=itr, col = 'red')
NeuronAcc <- read.csv("~/HeartDisease/Structure.csv", sep=";")
View(NeuronAcc)
View(heart)
train_labels <- read.csv("~/Hology1/training/crime/train_labels.csv")
View(train_labels)
library(dplyr)
library(ggplot2)
library(microbenchmark)
count(train_labels, class)
test_labels <- read.csv("~/Hology1/training/crime/test_labels.csv")
View(test_labels)
count(test_labels, class)
install.packages("keras")
package("keras")
library(keras)
getwd()
getwd()
x <- rnorm(1000, mean = 10, sd = 0.5)
t.test(x, mu = 10)
exit()
exit
q()
## P-values
# The Earth is Round (p < .05)
# Compare what we calculated to our hypothetical distribution and see if
# the value is "extreme" (p-value)
pt(0.8, 15, lower.tail = F) # the probability of seeing evidence as extreme or
# more extreme than that actually obtained under H0
choose(8, 7) * 0.5^8 + choose(8, 8) * 0.5^8
pbinom(6, size = 8, prob = .5, lower.tail = F) # >= 7
ppois(9, 5, lower.tail = F) # >= 10
pnorm(-0.355, lower.tail = F)
## Power
sigma <- 10
mu_0 <- 0
mu_a <- 2
n <- 100
alpha <- .05
plot(c(-3, 6), c(0, dnorm(0)), type = "n", frame = F, xlab = "Z value", ylab = "")
xvals <- seq(-3, 6, length = 1000)
lines(xvals, dnorm(xvals), type = "l", lwd = 3)
lines(xvals, dnorm(xvals, mean = sqrt(n)*(mu_a - mu_0) / sigma), lwd = 3)
abline(v = qnorm(1 - alpha))
# power.t.test -> T-test power
power.t.test(n = 16, delta = 2 / 4, sd = 1, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power
power.t.test(power = .8, delta = 2 / 4, sd = 1, type = "one.sample", alt = "one.sided")$n
power.t.test(power = .8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
power.t.test(power = .8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
## Multiple testing
# Error measure, correction
# no true positives
set.seed(1010093)
pValues <- rep(NA, 1000)
for(i in 1:1000) {
y <- rnorm(20)
x <- rnorm(20)
pValues[i] <- summary(lm(y ~ x))$coeff[2, 4]
}
sum(pValues < 0.05) # 51
# Controls FWER: Family wise error rate
sum(p.adjust(pValues, method = "bonferroni") < 0.05)
# Controls FDR: False discovery rate
sum(p.adjust(pValues, method = "BH") < 0.05)
# 50% true positives
set.seed(1010093)
pValues <- rep(NA, 1000)
for(i in 1:1000) {
x <- rnorm(20)
# first 500 beta = 0, last 500 beta = 2
if (i <= 500) {
y <- rnorm(20)
} else {
y <- rnorm(20, mean = 2*x)
}
pValues[i] <- summary(lm(y ~ x))$coeff[2, 4]
}
trueStatus <- rep(c("zero", "not zero"), each = 500)
table(pValues < 0.05, trueStatus)
# Controls FWER: Family wise error rate
table(p.adjust(pValues, method = "bonferroni") < 0.05, trueStatus)
# Controls FDR: False discovery rate
table(p.adjust(pValues, method = "BH") < 0.05, trueStatus)
par(mfrow = c(1, 2))
plot(pValues, p.adjust(pValues, method = "bonferroni"), pch = 19)
plot(pValues, p.adjust(pValues, method = "BH"), pch = 19)
## Resampled inference
# The jackknife: a tool for estimating standard errors and the bias of estimators
# Estimate the bias and standard error of the median
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
theta <- median(x)
jk <- sapply(1 : n, function(i) median(x[-i]))
thetaBar <- mean(jk)
biasEst <- (n - 1)*(thetaBar - theta)
seEst <- sqrt((n - 1)*mean((jk - thetaBar) ^ 2))
c(biasEst, seEst) # 0.0000000 0.1014066
library(bootstrap)
temp <- jackknife(x, median)
c(temp$jack.bias, temp$jack.se) # 0.0000000 0.1014066
## Bootstrapping
B <- 1000
resamples <- matrix(sample(x, n*B, replace = T), B, n)
medians <- apply(resamples, 1, median)
sd(medians)
quantile(medians, c(.025, .975))
hist(medians)
## Permutation tests
data(InsectSprays)
boxplot(count ~ spray, data = InsectSprays)
subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"), ]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
observedStat <- testStat(y, group)
permutations <- sapply(1 : 10000, function(i) testStat(y, sample(group)))
observedStat # 13.25
mean(permutations > observedStat) # 0
# p-value = 0 here
hist(permutations, breaks = 30
q()
install.packages(bootstrap)
install.packages(c("GGally", "gridExtra"))
source('~/.active-rstudio-document', echo=TRUE)
reticulate::repl_python()
import math
yes
clear
reticulate::repl_python()
install.packages("plotly")
install.packages("leaflet")
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
#addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
m  # Print the map
library(leaflet)
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
#addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
# Print the map
library(leaflet)
m <- leaflet() %>%
addTiles()   # Add default OpenStreetMap map tiles
#addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
m  # Print the map
library(leaflet)
m <- leaflet() %>%
addTiles()  %>% # Add default OpenStreetMap map tiles
addAwesomeMarkers(lng=83.30, lat=20.40, popup="My birth place") %>%
addMeasure(map, position = "topright", primaryLengthUnit = "feet", secondaryLengthUnit = NULL, primaryAreaUnit = "acres", secondaryAreaUnit = NULL, activeColor = "#ABE67E", completedColor = "#C8F2BE", popupOptions = list(className = "leaflet-measure-resultpopup", autoPanPadding = c(10, 10)), captureZIndex = 10000, localization = "en", decPoint = ".", thousandsSep = ",")
m # Print the map
library(leaflet)
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R") %>%
setView(lng = 174.768, lat = -36.852, zoom = 12)
m  # Print the map
setwd("~/DataScienceCoursera/PracticalML")
pml.testing <- read.csv("~/DataScienceCoursera/PracticalML/pml-testing.csv")
View(pml.testing)
pml.training <- read.csv("~/DataScienceCoursera/PracticalML/pml-training.csv", dec=",")
View(pml.training)
pml.training <- read.csv("~/DataScienceCoursera/PracticalML/pml-training.csv")
View(pml.training)
pml.testing <- read.csv("~/DataScienceCoursera/PracticalML/pml-testing.csv")
View(pml.testing)
pml.training$classe
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
na_info <- colSums(is.na(training))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
data1 <- data[, setdiff(x = names(data), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(data1))))
cat(paste("\n records = ", nrow(data1)))
cat(paste("\n variables =", ncol(data1)))
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
na_info <- colSums(is.na(training))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
data1 <- training[, setdiff(x = names(data), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(data1))))
cat(paste("\n records = ", nrow(data1)))
cat(paste("\n variables =", ncol(data1)))
c
summary(data1)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
na_info <- colSums(is.na(training))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
data1 <- training[, setdiff(x = names(training), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(data1))))
cat(paste("\n records = ", nrow(data1)))
cat(paste("\n variables =", ncol(data1)))
summary(data1)
library(h2o)
install.packages("h2o")
library(h2o)
h2o.init(ip = 'localhost', port = 5432, max_mem_size = '1g')
library(h2o)
h2o.init(ip = 'localhost', port = 5432, max_mem_size = '1g')
data.hex <- as.h2o(x = data1, destination_frame = "data.hex")
head(data.hex)
h2o.init(ip = 'localhost', port = 1080, max_mem_size = '1g')
data.hex <- as.h2o(x = data1, destination_frame = "data.hex")
head(data.hex)
View(data.hex)
View(data.hex)
data.hex
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
#Cleaning the training data.
na_info <- colSums(is.na(training))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
trainingData <- training[, setdiff(x = names(training), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(trainingData))))
cat(paste("\n records = ", nrow(trainingData)))
cat(paste("\n variables =", ncol(trainingData)))
summary(trainingData)
library(h2o)
h2o.init(ip = 'localhost', port = 1080, max_mem_size = '1g')
trainingData.hex <- as.h2o(x = trainingData, destination_frame = "trainingData.hex")
head(trainingData.hex)
#Cleaning the testing data.
na_info <- colSums(is.na(testing))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
testingData <- testing[, setdiff(x = names(testing), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(testingData))))
cat(paste("\n records = ", nrow(testingData)))
cat(paste("\n variables =", ncol(testingData)))
summary(testingData)
library(h2o)
h2o.init(ip = 'localhost', port = 1080, max_mem_size = '1g')
testingData.hex <- as.h2o(x = testingData, destination_frame = "testingData.hex")
head(testingData.hex)
View(trainingData)
View(trainingData)
View(testingData)
View(testingData)
summary(testingData)
knitr::opts_chunk$set(echo = TRUE)
#For training set
classe <- trainingData$classe
trainRemove <- grepl("^X|timestamp|window", names(trainingData))
trainingData <- trainingData[, !trainRemove]
trainCleaned <- trainingData[, sapply(trainingData, is.numeric)]
trainCleaned$classe <- classe
#For test set
testRemove <- grepl("^X|timestamp|window", names(testRaw))
#For training set
classe <- trainingData$classe
trainRemove <- grepl("^X|timestamp|window", names(trainingData))
trainingData <- trainingData[, !trainRemove]
trainCleaned <- trainingData[, sapply(trainingData, is.numeric)]
trainCleaned$classe <- classe
#For test set
testRemove <- grepl("^X|timestamp|window", names(testingData))
testingData <- testingData[, !testRemove]
testCleaned <- testingData[, sapply(testingData, is.numeric)]
dim(testCleaned)
dim(trainCleaned)
View(testCleaned)
View(testCleaned)
View(trainCleaned)
View(trainCleaned)
trainCleaned$classe <- classe
library(randomForest)
smp_size <- floor(0.70 * nrow(trainCleaned))
inTrain <- sample(seq_len(nrow(trainCleaned)), size = smp_size)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
model1 <- randomForest(Condition ~ ., data = trainData, importance = TRUE)
library(randomForest)
model1 <- randomForest(Condition ~ ., data = trainData, importance = TRUE)
library(randomForest)
model1 <- randomForest(classe ~ ., data = trainData, importance = TRUE)
library(randomForest)
model1 <- randomForest(trainData$classe ~ ., data = trainData, importance = TRUE)
print(model1)
library(randomForest)
model1 <- randomForest(trainData$classe ~ ., data = trainData, ntree = 500,      mtry = 6, importance = TRUE)
print(model1)
library(randomForest)
model <- randomForest(trainData$classe ~ ., data = trainData, ntree = 500,      mtry = 6, importance = TRUE)
print(model)
predictRf <- predict(model, testData)
confusionMatrix(testData$classe, predictRf)
predValid <- predict(model, testData, type = "class")
mean(predValid == testData$classe)
table(predValid,testData$classe)
predValid <- predict(model, testData, type = "class")
print("Model accuracy = ")
mean(predValid == testData$classe)*100
table(predValid,testData$classe)
predValid <- predict(model, testData, type = "class")
print("Model accuracy = ", mean(predValid == testData$classe)*100)
predValid <- predict(model, testData, type = "class")
print("Model accuracy = "+ mean(predValid == testData$classe)*100)
predValid <- predict(model, testData, type = "class")
print("Model accuracy = "+ str(mean(predValid == testData$classe)*100))
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %i", ValAcc)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(ConfusionMatrix,testData$classe)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
print("######################################################")
print(predValid)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
print("######################################################")
result <- predict(model, testCleaned[, -length(names(testCleaned))])
result
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot(corrPlot, method="color")
result <- predict(model, validationCleaned[, -length(names(validationCleaned))])
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
#Cleaning the training data.
na_info <- colSums(is.na(training))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
trainingData <- training[, setdiff(x = names(training), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(trainingData))))
cat(paste("\n records = ", nrow(trainingData)))
cat(paste("\n variables =", ncol(trainingData)))
summary(trainingData)
#Cleaning the testing data.
na_info <- colSums(is.na(testing))
length(names(na_info[na_info > 0]))
length(names(na_info[na_info > 19000]))
na_var <- names((na_info[na_info > 0]))
cat(paste("Type of na_var = ", class(na_var)))
testingData <- testing[, setdiff(x = names(testing), y = na_var)]
cat(paste("\n NA Count = ", sum(is.na(testingData))))
cat(paste("\n records = ", nrow(testingData)))
cat(paste("\n variables =", ncol(testingData)))
summary(testingData)
smp_size <- floor(0.70 * nrow(trainCleaned))
inTrain <- sample(seq_len(nrow(trainCleaned)), size = smp_size)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
library(randomForest)
model <- randomForest(trainData$classe ~ ., data = trainData, ntree = 500, mtry = 6, importance = TRUE)
print(model)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
result <- predict(model, validationCleaned[, -length(names(validationCleaned))])
print(result)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
#For training set
classe <- trainingData$classe
trainRemove <- grepl("^X|timestamp|window", names(trainingData))
trainingData <- trainingData[, !trainRemove]
trainCleaned <- trainingData[, sapply(trainingData, is.numeric)]
trainCleaned$classe <- classe
#For validation set
validationRemove <- grepl("^X|timestamp|window", names(validationData))
validationData <- validationData[, !validationRemove]
validationCleaned <- validationData[, sapply(validationData, is.numeric)]
smp_size <- floor(0.70 * nrow(trainCleaned))
inTrain <- sample(seq_len(nrow(trainCleaned)), size = smp_size)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
library(randomForest)
model <- randomForest(trainData$classe ~ ., data = trainData, ntree = 500, mtry = 6, importance = TRUE)
print(model)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
result <- predict(model, validationCleaned[, -length(names(validationCleaned))])
print(result)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
#For training set
classe <- trainingData$classe
trainRemove <- grepl("^X|timestamp|window", names(trainingData))
trainingData <- trainingData[, !trainRemove]
trainCleaned <- trainingData[, sapply(trainingData, is.numeric)]
trainCleaned$classe <- classe
#For validation set
validationRemove <- grepl("^X|timestamp|window", names(validationData))
validationData <- validationData[, !validationRemove]
validationCleaned <- validationData[, sapply(validationData, is.numeric)]
smp_size <- floor(0.70 * nrow(trainCleaned))
inTrain <- sample(seq_len(nrow(trainCleaned)), size = smp_size)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
library(randomForest)
model <- randomForest(trainData$classe ~ ., data = trainData, ntree = 500, mtry = 6, importance = TRUE)
print(model)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
result <- predict(model, validationCleaned[, -length(names(validationCleaned))])
print(result)
training <- read.csv("pml-training.csv")
na_info <- colSums(is.na(training))
na_var <- names((na_info[na_info > 0]))
trainingData <- training[, setdiff(x = names(training), y = na_var)]
dim(trainingData)
validation <- read.csv("pml-testing.csv")
na_info <- colSums(is.na(validation))
na_var <- names((na_info[na_info > 0]))
validationData <- validation[, setdiff(x = names(validation), y = na_var)]
dim(validationData)
#For training set
classe <- trainingData$classe
trainRemove <- grepl("^X|timestamp|window", names(trainingData))
trainingData <- trainingData[, !trainRemove]
trainCleaned <- trainingData[, sapply(trainingData, is.numeric)]
trainCleaned$classe <- classe
#For validation set
validationRemove <- grepl("^X|timestamp|window", names(validationData))
validationData <- validationData[, !validationRemove]
validationCleaned <- validationData[, sapply(validationData, is.numeric)]
smp_size <- floor(0.70 * nrow(trainCleaned))
inTrain <- sample(seq_len(nrow(trainCleaned)), size = smp_size)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
library(randomForest)
model <- randomForest(trainData$classe ~ ., data = trainData, ntree = 500, mtry = 6, importance = TRUE)
print(model)
predValid <- predict(model, testData, type = "class")
ValAcc <- mean(predValid == testData$classe)*100
sprintf("Validation Accuracy of the model is:- %f", ValAcc)
table(predValid,testData$classe)
result <- predict(model, validationCleaned[, -length(names(validationCleaned))])
print(result)
